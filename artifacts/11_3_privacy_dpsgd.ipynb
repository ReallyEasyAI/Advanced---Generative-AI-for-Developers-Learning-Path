{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwDK47gfLsYf"
   },
   "source": [
    "# Implement Differential Privacy with TensorFlow Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "* Learn how to wrap existing optimizers (e.g., SGD, Adam) into their differentially private counterparts using TensorFlow Privacy\n",
    "* Understand hyperparameters introduced by differentially private machine learning\n",
    "* Measure the privacy guarantee provided using analysis tools included in TensorFlow Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00fQV7e0Unz3"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsCUvXP0W4j2"
   },
   "source": [
    "[Differential privacy](https://en.wikipedia.org/wiki/Differential_privacy) (DP) is a framework for measuring the privacy guarantees provided by an algorithm. Through the lens of differential privacy, you can design machine learning algorithms that responsibly train models on private data. Learning with differential privacy provides measurable guarantees of privacy, helping to mitigate the risk of exposing sensitive training data in machine learning. Intuitively, a model trained with differential privacy should not be affected by any single training example, or small set of training examples, in its data set. This helps mitigate the risk of exposing sensitive training data in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vd8qUwEW5pP"
   },
   "source": [
    "The basic idea of this approach, called differentially private stochastic gradient descent (DP-SGD), is to modify the gradients\n",
    "used in stochastic gradient descent (SGD), which lies at the core of almost all deep learning algorithms. Models trained with DP-SGD provide provable differential privacy guarantees for their input data. There are two modifications made to the vanilla SGD algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUphKzYu01O9"
   },
   "source": [
    "1. First, the sensitivity of each gradient needs to be bounded. In other words, you need to limit how much each individual training point sampled in a minibatch can influence gradient computations and the resulting updates applied to model parameters. This can be done by *clipping* each gradient computed on each training point.\n",
    "2. *Random noise* is sampled and added to the clipped gradients to make it statistically impossible to know whether or not a particular data point was included in the training dataset by comparing the updates SGD applies when it operates with or without this particular data point in the training dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXU7MZhhW-aL"
   },
   "source": [
    "This tutorial uses [tf.keras](https://www.tensorflow.org/guide/keras) to train a convolutional neural network (CNN) to recognize handwritten digits with the DP-SGD optimizer provided by the TensorFlow Privacy library. TensorFlow Privacy provides code that wraps an existing TensorFlow optimizer to create a variant that implements DP-SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijJYKVc05DYX"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "r56BqqyEqA16",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-privacy==0.8.12\n",
      "  Downloading tensorflow_privacy-0.8.12-py3-none-any.whl.metadata (962 bytes)\n",
      "Collecting dp_accounting==0.4.3\n",
      "  Downloading dp_accounting-0.4.3-py3-none-any.whl.metadata (1.8 kB)\n",
      "Downloading tensorflow_privacy-0.8.12-py3-none-any.whl (405 kB)\n",
      "Downloading dp_accounting-0.4.3-py3-none-any.whl (104 kB)\n",
      "Installing collected packages: tensorflow-privacy, dp_accounting\n",
      "Successfully installed dp_accounting-0.4.3 tensorflow-privacy-0.8.12\n"
     ]
    }
   ],
   "source": [
    "# In a Jupyter notebook, the exclamation mark (!) indicates \n",
    "# that this line is executed as a shell command rather than Python code.\n",
    "# \n",
    "# Here we are installing two specific Python packages:\n",
    "#   1) tensorflow-privacy (version 0.8.12)\n",
    "#   2) dp_accounting (version 0.4.3)\n",
    "#\n",
    "# The --user flag installs the packages into the user's home directory \n",
    "# rather than at the system level, preventing potential permission issues.\n",
    "#\n",
    "# The --no-deps (no dependencies) flag tells pip not to install any \n",
    "# additional packages that these libraries might depend on. We might do this \n",
    "# if we want to strictly manage dependency versions ourselves. \n",
    "#\n",
    "# Note: In a managed notebook environment like Vertex AI, ensure that your \n",
    "# environment is set up to allow user installations before running this command.\n",
    "\n",
    "!pip install --user --no-deps tensorflow-privacy==0.8.12 dp_accounting==0.4.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKuHPYQCsV-x"
   },
   "source": [
    "Begin by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We import the os and warnings modules. \n",
    "#   - The 'os' module provides functions for interacting with the operating system.\n",
    "#   - The 'warnings' module helps manage and filter Python warning messages.\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# We set an environment variable 'TF_CPP_MIN_LOG_LEVEL' to '2'.\n",
    "# This environment variable is used by TensorFlow to control the verbosity \n",
    "# of log messages printed to the console.\n",
    "# Possible values and their effects:\n",
    "#   0: All logs are shown (includes debug messages).\n",
    "#   1: Filter out INFO logs.\n",
    "#   2: Filter out INFO and WARNING logs (only errors are shown).\n",
    "#   3: Filter out all logs (errors, warnings, and info).\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# We use the 'warnings.filterwarnings' function to ignore all warning messages.\n",
    "# This can help keep the notebook output clean, but be cautious because \n",
    "# ignoring warnings might mean missing important information about potential issues.\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ef56gCUqrdVn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the numpy library and alias it as np.\n",
    "# This is a common Python convention so that when we use numpy, \n",
    "# we can easily refer to it as np instead of typing numpy every time.\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and alias it as tf.\n",
    "# TensorFlow is a popular machine learning framework developed by Google.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Here we are setting the TensorFlow logger's log level to \"ERROR\". \n",
    "# This means that only error messages will be printed, \n",
    "# and warnings or informational messages will be suppressed.\n",
    "tf.get_logger().setLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_fVhfUyeI3d"
   },
   "source": [
    "Import TensorFlow Privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RseeuA7veIHU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# DIFFERENTIAL PRIVACY\n",
    "# -------------------------------------------------------------------------\n",
    "# Differential Privacy is a framework that helps ensure the privacy of\n",
    "# individual data points in a dataset. The main idea is to limit how much\n",
    "# any single data point (e.g., a person's record) can affect the outcome\n",
    "# of a computation, such as the training of a machine learning model.\n",
    "#\n",
    "# In practice, one way to achieve differential privacy is through\n",
    "# Differentially Private Stochastic Gradient Descent (DP-SGD). DP-SGD\n",
    "# modifies the standard SGD procedure by:\n",
    "#   1) Clipping the gradients for each individual sample or microbatch\n",
    "#      (so that no single data point can excessively influence the update).\n",
    "#   2) Adding random noise to those clipped gradients to mask the \n",
    "#      contribution of individual samples.\n",
    "#\n",
    "# These steps ensure that the final trained model does not overly \n",
    "# \"memorize\" any single data record, thereby protecting the privacy of \n",
    "# individuals in the dataset. \n",
    "#\n",
    "# -------------------------------------------------------------------------\n",
    "# EPSILON (ε) AND DELTA (δ)\n",
    "# -------------------------------------------------------------------------\n",
    "# When discussing differential privacy, you will often hear about ε (epsilon)\n",
    "# and δ (delta). They represent the \"privacy budget\" of your training:\n",
    "#\n",
    "#   - ε (epsilon) is the core measure of how much your model's outputs\n",
    "#     could potentially differ if a single individual’s data were removed.\n",
    "#     A smaller ε means stronger privacy guarantees, but potentially \n",
    "#     lower model accuracy.\n",
    "#\n",
    "#   - δ (delta) is a small probability that allows the differential privacy\n",
    "#     guarantee to be broken. In simpler terms, it captures rare \"worst-case\"\n",
    "#     scenarios. Typically, δ is set to be smaller than 1/N, where N is \n",
    "#     the size of the dataset.\n",
    "#\n",
    "# -------------------------------------------------------------------------\n",
    "# compute_dp_sgd_privacy\n",
    "# -------------------------------------------------------------------------\n",
    "# The 'compute_dp_sgd_privacy' function in 'tensorflow_privacy' helps \n",
    "# analyze the privacy budget (ε, δ) given:\n",
    "#   - The number of training steps (how many times the model updates),\n",
    "#   - The batch size and total dataset size (to understand the sampling rate),\n",
    "#   - The noise multiplier (how much noise is added to the gradients),\n",
    "#   - And sometimes the clipping norm (how much the gradients are clipped).\n",
    "#\n",
    "# By providing these hyperparameters, 'compute_dp_sgd_privacy' can \n",
    "# estimate the overall privacy budget (ε, δ) used by your model. This \n",
    "# helps you know how well your model is protecting individuals' data.\n",
    "#\n",
    "# -------------------------------------------------------------------------\n",
    "# IMPORT STATEMENTS\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# The 'tensorflow_privacy' library extends TensorFlow with tools \n",
    "# that enable differentially private training of machine learning models.\n",
    "import tensorflow_privacy\n",
    "\n",
    "# Here, we specifically import the function 'compute_dp_sgd_privacy'\n",
    "# from the 'privacy.analysis' module within 'tensorflow_privacy'. \n",
    "# This function will be used to estimate the (ε, δ) privacy guarantees\n",
    "# after training a model with DP-SGD.\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mU1p8N7M5Mmn"
   },
   "source": [
    "## Load and pre-process the dataset\n",
    "\n",
    "Load the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset and prepare the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_1ML23FlueTr",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# MNIST DATASET LOADING AND PREPROCESSING\n",
    "# -----------------------------------------------------------------------------\n",
    "# The MNIST dataset consists of 70,000 handwritten digit images (28x28 pixels),\n",
    "# each labeled with its corresponding digit (0 through 9).\n",
    "# Typically, 60,000 images are used for training, and 10,000 for testing.\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the MNIST dataset using TensorFlow Keras utilities.\n",
    "# The returned objects are tuples (images, labels) for both training and testing.\n",
    "# train -> (train_data, train_labels)\n",
    "# test  -> (test_data, test_labels)\n",
    "train, test = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# We then unpack the tuples into separate variables for clarity.\n",
    "train_data, train_labels = train\n",
    "test_data, test_labels = test\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# DATA NORMALIZATION (SCALING PIXELS)\n",
    "# -----------------------------------------------------------------------------\n",
    "# The pixel values in the MNIST images are originally integers in the range [0, 255].\n",
    "# We convert them to float32 (a common data type for deep learning) and scale \n",
    "# them to the range [0, 1]. This makes training more stable and efficient.\n",
    "train_data = np.array(train_data, dtype=np.float32) / 255.0\n",
    "test_data = np.array(test_data, dtype=np.float32) / 255.0\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# RESHAPING DATA FOR CONVOLUTIONAL NETWORKS\n",
    "# -----------------------------------------------------------------------------\n",
    "# If we're planning to use Convolutional Neural Networks (CNNs), we often need\n",
    "# the data to have a shape of (samples, height, width, channels). \n",
    "# Since MNIST is grayscale, channels = 1.\n",
    "# Here, each image is 28x28 pixels, so we reshape the data accordingly.\n",
    "train_data = train_data.reshape(train_data.shape[0], 28, 28, 1)\n",
    "test_data = test_data.reshape(test_data.shape[0], 28, 28, 1)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONVERTING LABELS TO CATEGORICAL (ONE-HOT) ENCODING\n",
    "# -----------------------------------------------------------------------------\n",
    "# MNIST labels are integers from 0 to 9. Many deep learning frameworks\n",
    "# expect labels in a one-hot encoded format, where each label is represented \n",
    "# by a vector of length 10 (for digits 0 to 9). \n",
    "# For example, the digit 2 would become [0, 0, 1, 0, 0, 0, 0, 0, 0, 0].\n",
    "train_labels = np.array(train_labels, dtype=np.int32)\n",
    "test_labels = np.array(test_labels, dtype=np.int32)\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ASSERTIONS FOR DATA VALIDATION\n",
    "# -----------------------------------------------------------------------------\n",
    "# We use Python assert statements to verify that our data has been \n",
    "# scaled correctly to the range [0, 1].\n",
    "# These checks will raise an error if the condition is not satisfied.\n",
    "assert train_data.min() == 0.0\n",
    "assert train_data.max() == 1.0\n",
    "assert test_data.min() == 0.0\n",
    "assert test_data.max() == 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVDcswOCtlr3"
   },
   "source": [
    "## Define the hyperparameters\n",
    "Set learning model hyperparamter values. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXNp_25y7JP2"
   },
   "source": [
    "DP-SGD has three general hyperamater and three privacy-specific hyperparameters that you must tune:\n",
    "\n",
    "**General hyperparameters**\n",
    "\n",
    "1. `epochs` (int) - This refers to the one entire passing of training data through the algorithm. Larger epoch increase the privacy risks since the model is trained on a same data point for multiple times.\n",
    "2. `batch_size` (int) - Batch size affects different aspects of DP-SGD training. For instance, increasing the batch size could reduce the amount of noise added during training under the same privacy guarantee, which reduces the training variance.\n",
    "3. `learning_rate` (float) - This hyperparameter already exists in vanilla SGD. The higher the learning rate, the more each update matters. If the updates are noisy (such as when the additive noise is large compared to the clipping threshold), a low learning rate may help the training procedure converge. \n",
    "\n",
    "**Privacy-specific hyperparameters**\n",
    "1. `l2_norm_clip` (float) - The maximum Euclidean (L2) norm of each gradient that is applied to update model parameters. This hyperparameter is used to bound the optimizer's sensitivity to individual training points. \n",
    "2. `noise_multiplier` (float) - Ratio of the standard deviation to the clipping norm (The amount of noise sampled and added to gradients during training). Generally, more noise results in better privacy (often, but not necessarily, at the expense of lower utility).\n",
    "3.   `microbatches` (int) - Each batch of data is split in smaller units called microbatches. By default, each microbatch should contain a single training example. This allows us to clip gradients on a per-example basis rather than after they have been averaged across the minibatch. This in turn decreases the (negative) effect of clipping on signal found in the gradient and typically maximizes utility. However, computational overhead can be reduced by increasing the size of microbatches to include more than one training examples. The average gradient across these multiple training examples is then clipped. The total number of examples consumed in a batch, i.e., one step of gradient descent, remains the same. The number of microbatches should evenly divide the batch size. \n",
    "\n",
    "\n",
    "Use the hyperparameter values below to obtain a reasonably accurate model (95% test accuracy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pVw_r2Mq7ntd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# HYPERPARAMETERS FOR TRAINING\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) epochs: How many times the model will see the entire training dataset.\n",
    "# 2) batch_size: Number of samples processed before the model updates its parameters.\n",
    "# 3) learning_rate: Controls how big a step the training algorithm takes \n",
    "#    when updating model parameters.\n",
    "#\n",
    "# In differential privacy terms, we often need two additional parameters:\n",
    "# 4) l2_norm_clip: Maximum value to which gradients are clipped to limit \n",
    "#    the influence of any single example.\n",
    "# 5) noise_multiplier: Amount of noise added to the clipped gradients \n",
    "#    to mask contributions of individual data points.\n",
    "# 6) num_microbatches: The number of subdivisions of a batch. Each microbatch\n",
    "#    is processed separately for clipping and then aggregated. Sometimes set \n",
    "#    to be the same as the batch size (meaning no microbatch splitting).\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 32\n",
    "learning_rate = 0.25\n",
    "\n",
    "l2_norm_clip = 1.0\n",
    "noise_multiplier = 0.5\n",
    "num_microbatches = 32  # Same as the batch size (i.e., no separate microbatches)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# VALIDATION: BATCH SIZE VERSUS MICROBATCHES\n",
    "# -----------------------------------------------------------------------------\n",
    "# For DP-SGD to function properly, the batch size should be an integer multiple \n",
    "# of the number of microbatches. If it's not, we raise a ValueError.\n",
    "#\n",
    "# In this example, num_microbatches = batch_size, so batch_size % num_microbatches \n",
    "# should be 0. If it isn't, there's likely a mismatch in how you've set up your \n",
    "# microbatching.\n",
    "# -----------------------------------------------------------------------------\n",
    "if batch_size % num_microbatches != 0:\n",
    "    raise ValueError(\n",
    "        \"Batch size should be an integer multiple of the number of microbatches.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXAmHcNOmHc5"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "Define a convolutional neural network as the learning model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oCOo8aOLmFta",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# MODEL DEFINITION: A SIMPLE CONVOLUTIONAL NEURAL NETWORK (CNN)\n",
    "# -----------------------------------------------------------------------------\n",
    "# We use tf.keras.Sequential to build a feed-forward model layer by layer.\n",
    "# Each layer is applied in sequence to the output of the previous layer.\n",
    "# This CNN architecture is suitable for image classification on MNIST.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Conv2D Layer 1:\n",
    "        #   - 16 filters: The number of different kernel \"patterns\" the layer \n",
    "        #     will learn to detect.\n",
    "        #   - Kernel size of 8 (i.e., 8x8 filters).\n",
    "        #   - strides=2: The filter moves 2 pixels at a time along width and height, \n",
    "        #     reducing spatial dimensions faster.\n",
    "        #   - padding=\"same\": Pads the input so the output size remains \n",
    "        #     (input_size / stride) for each dimension (if perfectly divisible).\n",
    "        #   - activation=\"relu\": A common non-linear activation function \n",
    "        #     (Rectified Linear Unit).\n",
    "        #   - input_shape=(28, 28, 1): The shape of each MNIST image \n",
    "        #     (28x28, single grayscale channel).\n",
    "        # ---------------------------------------------------------------------\n",
    "        tf.keras.layers.Conv2D(\n",
    "            16,         # Number of filters\n",
    "            8,          # Kernel size\n",
    "            strides=2,  \n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "            input_shape=(28, 28, 1),\n",
    "        ),\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # MaxPool2D Layer 1:\n",
    "        #   - pool_size=2: Takes a 2x2 window and outputs the maximum value \n",
    "        #     within that window, reducing spatial dimensions by half (if stride=2).\n",
    "        #   - strides=1 here means the pooling window moves 1 step at a time,\n",
    "        #     which can help extract slightly overlapping features.\n",
    "        # ---------------------------------------------------------------------\n",
    "        tf.keras.layers.MaxPool2D(\n",
    "            pool_size=2, \n",
    "            strides=1\n",
    "        ),\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Conv2D Layer 2:\n",
    "        #   - 32 filters, each of size 4x4.\n",
    "        #   - strides=2: Further reduces the spatial dimensions.\n",
    "        #   - padding=\"valid\": No padding is added, so the output size \n",
    "        #     shrinks based on the kernel size.\n",
    "        #   - activation=\"relu\": Same ReLU activation for non-linearity.\n",
    "        # ---------------------------------------------------------------------\n",
    "        tf.keras.layers.Conv2D(\n",
    "            32,\n",
    "            4, \n",
    "            strides=2,\n",
    "            padding=\"valid\",\n",
    "            activation=\"relu\"\n",
    "        ),\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # MaxPool2D Layer 2:\n",
    "        #   - Another max pooling layer to reduce spatial size \n",
    "        #     (height/width of the feature maps).\n",
    "        #   - pool_size=2 with strides=1 again.\n",
    "        # ---------------------------------------------------------------------\n",
    "        tf.keras.layers.MaxPool2D(\n",
    "            pool_size=2, \n",
    "            strides=1\n",
    "        ),\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Flatten Layer:\n",
    "        #   - Converts the 2D feature maps (plus channel dimension) into \n",
    "        #     a 1D vector, preparing the data for the Dense (fully connected) layers.\n",
    "        # ---------------------------------------------------------------------\n",
    "        tf.keras.layers.Flatten(),\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Dense Layer 1:\n",
    "        #   - 32 neurons, each with 'relu' activation.\n",
    "        #   - This layer learns non-linear combinations of features extracted \n",
    "        #     by the convolutional layers.\n",
    "        # ---------------------------------------------------------------------\n",
    "        tf.keras.layers.Dense(\n",
    "            32, \n",
    "            activation=\"relu\"\n",
    "        ),\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Dense Layer 2 (Output Layer):\n",
    "        #   - 10 neurons, corresponding to the 10 classes of digits (0-9).\n",
    "        #   - 'softmax' activation ensures each of the 10 outputs is between 0 and 1, \n",
    "        #     summing up to 1. This can be interpreted as the probability of each class.\n",
    "        # ---------------------------------------------------------------------\n",
    "        tf.keras.layers.Dense(\n",
    "            10, \n",
    "            activation=\"softmax\"\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FT4lByFg-I_r"
   },
   "source": [
    "Define the optimizer and loss function for the learning model. Compute the loss as a vector of losses per-example rather than as the mean over a minibatch to support gradient manipulation over each training point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bqBvjCf5-ZXy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# DIFFERENTIALLY PRIVATE OPTIMIZER SETUP\n",
    "# -----------------------------------------------------------------------------\n",
    "# A key component of differentially private stochastic gradient descent (DP-SGD)\n",
    "# is the use of an optimizer that can handle gradient clipping and noise addition\n",
    "# to each gradient update. \n",
    "#\n",
    "# Here, we use 'DPKerasSGDOptimizer' from the 'tensorflow_privacy' library. \n",
    "# It overrides how gradients are calculated and applies the differential \n",
    "# privacy steps under the hood.\n",
    "\n",
    "optimizer = tensorflow_privacy.DPKerasSGDOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,           # Gradient clipping threshold\n",
    "    noise_multiplier=noise_multiplier,   # Noise level added to clipped gradients\n",
    "    num_microbatches=num_microbatches,   # Number of microbatches per batch\n",
    "    learning_rate=learning_rate,         # Standard SGD learning rate\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# LOSS FUNCTION\n",
    "# -----------------------------------------------------------------------------\n",
    "# We use 'CategoricalCrossentropy' for multi-class classification problems \n",
    "# (digits 0 through 9). \n",
    "# \n",
    "# Notice that 'reduction=tf.losses.Reduction.NONE' is used because, in DP-SGD,\n",
    "# we often compute the loss separately for each microbatch so that we can \n",
    "# clip each sample or microbatch's gradient individually. Then, we combine \n",
    "# the gradient updates and add noise. \n",
    "#\n",
    "# If we used 'reduction=\"sum\"' or 'reduction=\"mean\"', we would lose the \n",
    "# fine-grained control needed for differential privacy, because those \n",
    "# modes aggregate the loss before we can apply clipping and noise \n",
    "# at the microbatch level.\n",
    "\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "    reduction=tf.losses.Reduction.NONE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LI_3nXzEGmrP"
   },
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "z4iV03VqG1Bo",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 156s 82ms/step - loss: 0.7882 - accuracy: 0.8309 - val_loss: 0.8839 - val_accuracy: 0.8781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc4d4d33790>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# COMPILING THE MODEL\n",
    "# -----------------------------------------------------------------------------\n",
    "# 'model.compile' configures the learning process. Here we specify:\n",
    "#   1) optimizer: The object that applies the gradient updates. In this case, \n",
    "#      the DP-SGD optimizer from 'tensorflow_privacy', which clips and adds \n",
    "#      noise to gradients for differential privacy.\n",
    "#   2) loss: The objective function used to measure the difference between \n",
    "#      the model's predictions and the true labels. We use \n",
    "#      'CategoricalCrossentropy' with NO reduction for DP-SGD.\n",
    "#   3) metrics: Additional statistics we want to track. Here, we track \"accuracy\",\n",
    "#      which measures the percentage of correctly classified samples.\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# TRAINING THE MODEL\n",
    "# -----------------------------------------------------------------------------\n",
    "# 'model.fit' starts the actual training process. Key parameters include:\n",
    "#   - train_data, train_labels: The inputs (images) and targets (digit labels).\n",
    "#   - epochs: Number of times the entire training dataset passes through \n",
    "#     the network.\n",
    "#   - validation_data: A separate dataset (test_data, test_labels) used to \n",
    "#     evaluate how well the model generalizes, without updating weights.\n",
    "#   - batch_size: The number of samples processed before the DP-SGD optimizer \n",
    "#     updates the model parameters. This corresponds to the \"batch_size\" \n",
    "#     we set earlier, which is also the same as 'num_microbatches' in this example.\n",
    "#\n",
    "# The method returns a History object that contains information about \n",
    "# training and validation performance (e.g., loss and accuracy per epoch).\n",
    "model.fit(\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    epochs=epochs,\n",
    "    validation_data=(test_data, test_labels),\n",
    "    batch_size=batch_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kkzQH2LXNjF"
   },
   "source": [
    "## Measure the differential privacy guarantee\n",
    "\n",
    "Perform a privacy analysis to measure the DP guarantee achieved by a training algorithm. Knowing the level of DP achieved enables the objective comparison of two training runs to determine which of the two is more privacy-preserving. At a high level, the privacy analysis measures how much a potential adversary can improve their guess about properties of any individual training point by observing the outcome of the training procedure (e.g., model updates and parameters). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TL7_lX5sHCTI"
   },
   "source": [
    "This guarantee is sometimes referred to as the **privacy budget**. A lower privacy budget bounds more tightly an adversary's ability to improve their guess. This ensures a stronger privacy guarantee. Intuitively, this is because it is harder for a single training point to affect the outcome of learning: for instance, the information contained in the training point cannot be memorized by the ML algorithm and the privacy of the individual who contributed this training point to the dataset is preserved.\n",
    "\n",
    "In this tutorial, the privacy analysis is performed in the framework of Rényi Differential Privacy (RDP), which is a relaxation of pure DP based on [this paper](https://arxiv.org/abs/1702.07476) that is particularly well suited for DP-SGD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUEk25pgmnm-"
   },
   "source": [
    "Two metrics are used to express the DP guarantee of an ML algorithm:\n",
    "\n",
    "1.   Delta ($\\delta$) - Bounds the probability of the privacy guarantee not holding. A rule of thumb is to set it to be less than the inverse of the size of the training dataset. In this tutorial, it is set to $10^{-5}$ as the MNIST dataset has 60,000 training points.\n",
    "2.   Epsilon ($\\epsilon$) - This is the privacy budget. It measures the strength of the privacy guarantee (or maximum tolerance for revealing information on input data) by bounding how much the probability of a particular model output can vary by including (or excluding) a single training point. A smaller value for $\\epsilon$ implies a better privacy guarantee. However, the $\\epsilon$ value is only an upper bound and a large value could still mean good privacy in practice.\n",
    "\n",
    "For more detail about the mathematical definition of $(\\epsilon, \\delta)$-differential privacy, see the original [DP-SGD paper](https://arxiv.org/pdf/1607.00133.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PczVdKsGyRQM"
   },
   "source": [
    "Tensorflow Privacy provides a tool, `compute_dp_sgd_privacy`, to compute the value of $\\epsilon$ given a fixed value of $\\delta$ and the following hyperparameters from the training process:\n",
    "\n",
    "1.   The total number of points in the training data, `n`.\n",
    "2. The `batch_size`.\n",
    "3.   The `noise_multiplier`.\n",
    "4. The number of `epochs` of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ws8-nVuVDgtJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP-SGD performed over 60000 examples with 32 examples per iteration, noise\n",
      "multiplier 0.5 for 1 epochs without microbatching, and no bound on number of\n",
      "examples per user.\n",
      "\n",
      "This privacy guarantee protects the release of all model checkpoints in addition\n",
      "to the final model.\n",
      "\n",
      "Example-level DP with add-or-remove-one adjacency at delta = 1e-05 computed with\n",
      "RDP accounting:\n",
      "    Epsilon with each example occurring once per epoch:        10.726\n",
      "    Epsilon assuming Poisson sampling (*):                      3.800\n",
      "\n",
      "No user-level privacy guarantee is possible without a bound on the number of\n",
      "examples per user.\n",
      "\n",
      "(*) Poisson sampling is not usually done in training pipelines, but assuming\n",
      "that the data was randomly shuffled, it is believed the actual epsilon should be\n",
      "closer to this value than the conservative assumption of an arbitrary data\n",
      "order.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# COMPUTING THE PRIVACY STATEMENT\n",
    "# -----------------------------------------------------------------------------\n",
    "# The 'compute_dp_sgd_privacy_statement' function from 'tensorflow_privacy' \n",
    "# produces a human-readable message about the (ε, δ) guarantees of a \n",
    "# differentially private training configuration.\n",
    "#\n",
    "# PARAMETERS:\n",
    "#   - number_of_examples:     The total number of training samples (e.g., 60,000 for MNIST).\n",
    "#   - batch_size:            How many samples are processed in one training step.\n",
    "#   - noise_multiplier:      The amount of noise added to the gradients in DP-SGD.\n",
    "#   - used_microbatching:    Whether microbatching is used. If 'False', \n",
    "#                            microbatching is effectively disabled or equals the batch size.\n",
    "#   - num_epochs:            How many times the entire dataset is traversed during training.\n",
    "#   - delta:                 The δ (delta) parameter in differential privacy, \n",
    "#                            typically set to a small number (like 1e-5 or 1 / (training set size)).\n",
    "#\n",
    "# The returned statement typically indicates the effective ε (epsilon) \n",
    "# for the given DP-SGD hyperparameters, letting you know the approximate \n",
    "# privacy budget spent after training completes.\n",
    "#\n",
    "# EXAMPLE:\n",
    "#   \"DP-SGD with sampling rate = ..., noise_multiplier = ... and ... steps \n",
    "#    achieves (ε, δ)-DP for ε=..., δ=1e-5.\"\n",
    "#\n",
    "# This statement tells you the privacy level (ε) you have, assuming you \n",
    "# set δ to 1e-5.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "dpsgd_statement = compute_dp_sgd_privacy.compute_dp_sgd_privacy_statement(\n",
    "    number_of_examples=train_data.shape[0],  # total training examples (e.g., 60,000)\n",
    "    batch_size=batch_size,                  # batch size used during training\n",
    "    noise_multiplier=noise_multiplier,      # noise added to gradients\n",
    "    used_microbatching=False,               # 'False' means microbatching not in use\n",
    "    num_epochs=epochs,                      # how many epochs were trained\n",
    "    delta=1e-5                              # small delta for DP (1e-5 is commonly used)\n",
    ")\n",
    "\n",
    "# The returned string describes the resulting (ε, δ) privacy guarantee. \n",
    "# We print it out to see how strong our differential privacy is, \n",
    "# given the chosen hyperparameters.\n",
    "print(dpsgd_statement)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-KyttEWFRDc"
   },
   "source": [
    "The tool reports $\\epsilon$ value for the hyperparameters chosen above, including $\\delta=10^{-5}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2024 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "1. **What is being computed?**  \n",
    "   - The statement shows the privacy guarantees when using DP-SGD on 60,000 training examples (MNIST) with:\n",
    "     - A batch size of 32.  \n",
    "     - A noise multiplier of 0.5.  \n",
    "     - 1 epoch of training (the model saw each image once).\n",
    "\n",
    "2. **Example-Level DP**  \n",
    "   - *Example-level* differential privacy means each **individual image (example)** is protected. When you see “no user-level privacy guarantee,” it indicates there’s no upper limit on how many images could belong to the same user (i.e., a single user could contribute multiple images).\n",
    "\n",
    "3. **\\(\\varepsilon\\) (Epsilon) and \\(\\delta\\)**  \n",
    "   - The privacy level is measured by \\(\\varepsilon\\) (epsilon) and \\(\\delta\\).  \n",
    "   - **\\(\\delta = 1e-5\\)** is a small probability that the worst-case privacy guarantee might fail.  \n",
    "   - Two different epsilon values are shown:\n",
    "     - **Epsilon = 10.726** under the conservative assumption of arbitrary data order.  \n",
    "     - **Epsilon = 3.800** under the assumption of Poisson sampling (a more randomized data order).  \n",
    "   - A **lower** epsilon implies a **stronger** privacy guarantee.\n",
    "\n",
    "4. **Why Two Epsilon Values?**  \n",
    "   - DP-SGD accounting can be done in different ways:\n",
    "     - **Conservative assumption**: Data might be arranged in the worst possible order for privacy.  \n",
    "     - **Poisson sampling assumption**: The data was well-shuffled, and each example is included in a batch with some probability. This typically yields a **lower** (better) epsilon in practice.\n",
    "\n",
    "5. **No Bound on the Number of Examples per User**  \n",
    "   - Without a limit on how many samples each user can contribute, the statement notes there is no *user-level* differential privacy guarantee.  \n",
    "   - *User-level DP* would require each *user* to have a maximum number of samples in the dataset (so that bounding the user’s contributions could further protect that user’s data).\n",
    "\n",
    "6. **Interpretation**  \n",
    "   - Even though \\(\\varepsilon\\) might be somewhat large (like 10.726), the presence of noise still obscures individual contributions compared to training without DP.  \n",
    "   - If you want **stronger privacy guarantees** (i.e., a smaller epsilon), you could:\n",
    "     - Increase the noise multiplier,  \n",
    "     - Reduce the number of epochs, or  \n",
    "     - Use smaller batch sizes (increasing the sampling rate).\n",
    "\n",
    "Overall, this statement helps you **quantify** how strong your model’s privacy protection is under various assumptions about how data is sampled and organized.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "classification_privacy.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
