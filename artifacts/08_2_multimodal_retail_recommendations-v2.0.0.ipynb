{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijGzTHJJUCPY"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEqbX8OhE8y9"
   },
   "source": [
    "# Multimodal retail recommendation: using Gemini to recommend items based on images and image reasoning\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retail/multimodal_retail_recommendations.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fuse-cases%2Fretail%2Fmultimodal_retail_recommendations.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/retail/multimodal_retail_recommendations.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retail/multimodal_retail_recommendations.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retail/multimodal_retail_recommendations.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retail/multimodal_retail_recommendations.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retail/multimodal_retail_recommendations.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/X_logo_2023_original.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retail/multimodal_retail_recommendations.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retail/multimodal_retail_recommendations.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "048a84091064"
   },
   "source": [
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Thu Ya Kyaw](https://github.com/iamthuya) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VK1Q5ZYdVL4Y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "For retail companies, recommendation systems improve customer experience and thus can increase sales.\n",
    "\n",
    "This notebook shows how you can use the multimodal capabilities of Gemini 1.5 Pro model to rapidly create a multimodal recommendation system out-of-the-box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zLxCZkKEAhC"
   },
   "source": [
    "## Scenario\n",
    "\n",
    "The customer shows you their living room:\n",
    "\n",
    "|Customer photo |\n",
    "|:-----:|\n",
    "|<img src=\"https://storage.googleapis.com/github-repo/img/gemini/retail-recommendations/rooms/spacejoy-c0JoR_-2x3E-unsplash.jpg\" width=\"80%\">  |\n",
    "\n",
    "\n",
    "\n",
    "Below are four chair options that the customer is trying to decide between:\n",
    "\n",
    "|Chair 1| Chair 2 | Chair 3 | Chair 4 |\n",
    "|:-----:|:----:|:-----:|:----:|\n",
    "| <img src=\"https://storage.googleapis.com/github-repo/img/gemini/retail-recommendations/furnitures/cesar-couto-OB2F6CsMva8-unsplash.jpg\" width=\"80%\">|<img src=\"https://storage.googleapis.com/github-repo/img/gemini/retail-recommendations/furnitures/daniil-silantev-1P6AnKDw6S8-unsplash.jpg\" width=\"80%\">|<img src=\"https://storage.googleapis.com/github-repo/img/gemini/retail-recommendations/furnitures/ruslan-bardash-4kTbAMRAHtQ-unsplash.jpg\" width=\"80%\">|<img src=\"https://storage.googleapis.com/github-repo/img/gemini/retail-recommendations/furnitures/scopic-ltd-NLlWwR4d3qU-unsplash.jpg\" width=\"80%\">|\n",
    "\n",
    "\n",
    "How can you use Gemini 1.5 Pro, a multimodal model, to help the customer choose the best option, and also explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQT500QqVPIb"
   },
   "source": [
    "### Objectives\n",
    "\n",
    "Your main objective is to learn how to create a recommendation system that can provide both recommendations and explanations using a multimodal model: Gemini 1.5 Pro.\n",
    "\n",
    "In this notebook, you will begin with a scene (e.g. a living room) and use the Gemini 1.5 Pro model to perform visual understanding. You will also investigate how the Gemini Pro model can be used to recommend an item (e.g. a chair) from a list of furniture items as input.\n",
    "\n",
    "By going through this notebook, you will learn:\n",
    "- how to use the Gemini Pro model to perform visual understanding\n",
    "- how to take multimodality into consideration in prompting for the Gemini Pro model\n",
    "- how the Gemini Pro model can be used to create retail recommendation applications out-of-the-box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1y6_3dTwV2fI"
   },
   "source": [
    "### Costs\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "- Vertex AI\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDU0XJ1xRDlL"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5afkyDMSBW5"
   },
   "source": [
    "### Install Vertex AI SDK for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_-ThW_ZUYRV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --user google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### Restart current runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRvKdaPDTznN"
   },
   "outputs": [],
   "source": [
    "# Restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Fom0ZkMSBW6"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you are running this notebook on Google Colab, run the following cell to authenticate your environment. This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCaCx6PLSBW6"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Additional authentication is required for Google Colab\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Authenticate user to Google Cloud\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuQwwRiniVFG"
   },
   "source": [
    "### Define Google Cloud project information and initialize Vertex AI\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rtMowvm-yQ97",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We define two variables, PROJECT_ID and LOCATION, which are crucial to \n",
    "# identifying the Google Cloud project and the region where Vertex AI resources are located.\n",
    "# The strings \"[insert project here]\" and \"[insert location here]\" are placeholders.\n",
    "# Replace them with your actual GCP project ID and region (e.g., \"us-central1\").\n",
    "\n",
    "PROJECT_ID = \"[insert project here]\"  # @param {type:\"string\"}\n",
    "LOCATION = \"[insert location here]\"   # @param {type:\"string\"}\n",
    "\n",
    "# Next, we import the vertexai library so we can initialize it for our project.\n",
    "import vertexai\n",
    "\n",
    "# We initialize Vertex AI with the specified project and location.\n",
    "# This step ensures that all Vertex AI functionalities will be associated \n",
    "# with the correct resources in the specified project and region.\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4437b7608c8e"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yZ75ioBU9EwM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We import GenerativeModel and Image from the Vertex AI generative models library.\n",
    "# GenerativeModel is our primary interface for generative AI tasks (like text or image generation),\n",
    "# and Image is specifically designed for image-related operations.\n",
    "\n",
    "from vertexai.generative_models import (\n",
    "    GenerativeModel,  # The base interface for various generative tasks\n",
    "    Image             # A specialized class for handling image generation and manipulation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhcZv4HFUYRW"
   },
   "source": [
    "## Using Gemini 1.5 Pro model\n",
    "\n",
    "The Gemini 1.5 Pro model `gemini-1.5-pro` is a multimodal model that supports adding image and video in text or chat prompts for a text response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExdEEGUqUYRW"
   },
   "source": [
    "### Load Gemini 1.5 Pro model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pxlvLmncUYRW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We create an instance of a generative model named \"gemini-1.5-pro\".\n",
    "# This tells Vertex AI which specific model we want to use for our multimodal tasks.\n",
    "multimodal_model = GenerativeModel(\"gemini-1.5-pro\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNxn74L2UNdU"
   },
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VEUnTOHz95vm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We import modules required for handling and displaying images within a Jupyter environment.\n",
    "import http.client           # Enables us to handle low-level HTTP protocol requests and responses\n",
    "import io                    # Allows for in-memory binary I/O (BytesIO) operations\n",
    "import typing                # Provides type hints to help clarify function parameters and return types\n",
    "import urllib.request        # Helps us open and read URLs (for example, to retrieve images)\n",
    "\n",
    "import IPython.display       # Lets us display images or other media directly in a Jupyter environment\n",
    "from PIL import Image as PIL_Image       # A popular Python imaging library (Pillow) for image manipulation\n",
    "from PIL import ImageOps as PIL_ImageOps # Contains operations for image resizing, flipping, etc.\n",
    "\n",
    "\n",
    "def display_image(\n",
    "    image: Image,\n",
    "    max_width: int = 600,\n",
    "    max_height: int = 350\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    We display a Vertex AI Image object in the notebook while ensuring\n",
    "    it's converted to a standard RGB mode and resized if it exceeds\n",
    "    the specified dimensions.\n",
    "    \"\"\"\n",
    "    # We convert the underlying image to a PIL Image to allow manipulation.\n",
    "    pil_image = typing.cast(PIL_Image.Image, image._pil_image)\n",
    "\n",
    "    # We check if the image mode is not RGB (e.g., RGBA, CMYK), then convert\n",
    "    # it to RGB. Some Jupyter environments do not support alpha channels.\n",
    "    if pil_image.mode != \"RGB\":\n",
    "        pil_image = pil_image.convert(\"RGB\")\n",
    "\n",
    "    # We grab the current dimensions of the PIL image.\n",
    "    image_width, image_height = pil_image.size\n",
    "\n",
    "    # We check if the image exceeds our desired dimensions. If it does,\n",
    "    # we resize it while keeping the aspect ratio intact.\n",
    "    if max_width < image_width or max_height < image_height:\n",
    "        pil_image = PIL_ImageOps.contain(pil_image, (max_width, max_height))\n",
    "\n",
    "    # We hand the PIL image off to a helper function that compresses\n",
    "    # and displays the image inline in the Jupyter notebook.\n",
    "    display_image_compressed(pil_image)\n",
    "\n",
    "\n",
    "def display_image_compressed(pil_image: PIL_Image.Image) -> None:\n",
    "    \"\"\"\n",
    "    We compress and display a PIL Image object in a Jupyter notebook\n",
    "    to reduce file size while retaining decent image quality.\n",
    "    \"\"\"\n",
    "    # We create an in-memory binary stream where we can save the compressed image.\n",
    "    image_io = io.BytesIO()\n",
    "\n",
    "    # We save the image as a JPEG with 80% quality, which strikes a balance\n",
    "    # between visual fidelity and file size. 'optimize=True' helps reduce size further.\n",
    "    pil_image.save(image_io, \"jpeg\", quality=80, optimize=True)\n",
    "\n",
    "    # We retrieve the compressed bytes from the in-memory stream.\n",
    "    image_bytes = image_io.getvalue()\n",
    "\n",
    "    # We create an IPython Image object and display it inline in the notebook.\n",
    "    ipython_image = IPython.display.Image(image_bytes)\n",
    "    IPython.display.display(ipython_image)\n",
    "\n",
    "\n",
    "def get_image_bytes_from_url(image_url: str) -> bytes:\n",
    "    \"\"\"\n",
    "    We fetch raw image bytes from a given URL. This function raises an exception\n",
    "    if the server response indicates the content is not in PNG or JPEG format.\n",
    "    \"\"\"\n",
    "    # We open the URL and read the response into memory.\n",
    "    with urllib.request.urlopen(image_url) as response:\n",
    "        # We cast the response object for clarity when typing.\n",
    "        response = typing.cast(http.client.HTTPResponse, response)\n",
    "\n",
    "        # We ensure the returned data is either PNG or JPEG,\n",
    "        # otherwise we raise an error.\n",
    "        if response.headers[\"Content-Type\"] not in (\"image/png\", \"image/jpeg\"):\n",
    "            raise Exception(\"Image can only be in PNG or JPEG format\")\n",
    "\n",
    "        # We read all the raw bytes of the image from the response.\n",
    "        image_bytes = response.read()\n",
    "\n",
    "    # We return the entire byte array of the downloaded image.\n",
    "    return image_bytes\n",
    "\n",
    "\n",
    "def load_image_from_url(image_url: str) -> Image:\n",
    "    \"\"\"\n",
    "    We load an image from a given URL and convert it into\n",
    "    a Vertex AI Image object, which we can feed into generative models.\n",
    "    \"\"\"\n",
    "    # We first get the raw bytes for the image from the URL.\n",
    "    image_bytes = get_image_bytes_from_url(image_url)\n",
    "\n",
    "    # We then convert those raw bytes into a Vertex AI Image object.\n",
    "    return Image.from_bytes(image_bytes)\n",
    "\n",
    "\n",
    "def print_multimodal_prompt(contents: list):\n",
    "    \"\"\"\n",
    "    We iterate through a list of text strings and Image objects,\n",
    "    displaying images in the notebook and printing text to provide\n",
    "    a complete overview of what we're sending to the model.\n",
    "    \"\"\"\n",
    "    for content in contents:\n",
    "        # If the item is a Vertex AI Image, display it inline in the notebook.\n",
    "        if isinstance(content, Image):\n",
    "            display_image(content)\n",
    "        else:\n",
    "            # Otherwise, we treat it as text and simply print it.\n",
    "            print(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MONqa2K8IjSz"
   },
   "source": [
    "### Visual understanding with Gemini 1.5 Pro\n",
    "\n",
    "Here you will ask the Gemini 1.5 Pro model to describe a room in details from its image. To do that you have to **combine text and image in a single prompt**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26wP-epVFBBK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We define a URL that points to an image of a room. This image \n",
    "# will be used as a prompt for our multimodal model.\n",
    "room_image_url = (\n",
    "    \"https://storage.googleapis.com/github-repo/img/gemini/retail-recommendations/rooms/\"\n",
    "    \"spacejoy-c0JoR_-2x3E-unsplash.jpg\"\n",
    ")\n",
    "\n",
    "# We convert the above URL into a Vertex AI Image object so it can be passed to our model.\n",
    "room_image = load_image_from_url(room_image_url)\n",
    "\n",
    "# This prompt asks the model to describe what is visible in the room \n",
    "# and the overall atmosphere of the setting.\n",
    "prompt = \"Describe what's visible in this room and the overall atmosphere:\"\n",
    "\n",
    "# We combine our text prompt and the room image into one list, \n",
    "# so both the text and the image will be processed by the model.\n",
    "contents = [\n",
    "    prompt,\n",
    "    room_image,\n",
    "]\n",
    "\n",
    "# We call the 'generate_content' method on our multimodal model, \n",
    "# asking it to produce output (responses) in a streaming manner.\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "# We display the combined prompt, which includes the text prompt and the room image.\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "# Finally, we print the model's response. The 'end=\"\"' prevents \n",
    "# extra newlines from being inserted between streamed chunks.\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64I8-GxSbkTv"
   },
   "source": [
    "### Generating open recommendations based on built-in knowledge\n",
    "\n",
    "Using the same image, you can ask the model to recommend **a piece of furniture** that would fit in it alongside with the description of the room.\n",
    "\n",
    "Note that the model can choose **any furniture** to recommend in this case, and can do so from its only built-in knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UaRDl1WvypT8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We define two separate prompt strings:\n",
    "# 1) \"Recommend a new piece of furniture for this room:\"\n",
    "# 2) \"and explain the reason in detail\"\n",
    "# \n",
    "# Then, we combine these text prompts and the previously loaded 'room_image' into a single list.\n",
    "# The multimodal model will analyze both the text and the image together.\n",
    "\n",
    "prompt1 = \"Recommend a new piece of furniture for this room:\"\n",
    "prompt2 = \"and explain the reason in detail\"\n",
    "\n",
    "contents = [\n",
    "    prompt1,\n",
    "    room_image,\n",
    "    prompt2\n",
    "]\n",
    "\n",
    "# We generate content from our multimodal model in a streaming fashion.\n",
    "# As soon as the model starts producing partial responses, we can read them in real-time.\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "# For clarity, we first print the prompt contents (text + image) so we can see\n",
    "# exactly what the model is receiving.\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "# Then, we print the model's response. The 'end=\"\"' argument ensures that \n",
    "# we don't add unnecessary newlines between each streaming chunk.\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzdaG4iIdZM8"
   },
   "source": [
    "In the next cell, you will ask the model to recommend **a type of chair** that would fit in it alongside with the description of the room.\n",
    "\n",
    "Note that the model can choose **any type of chair** to recommend in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7xKDMFLyQuD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We define two separate prompt strings:\n",
    "# 1) \"Describe this room:\"\n",
    "# 2) \"and recommend a type of chair that would fit in it\"\n",
    "#\n",
    "# Then, we combine these text prompts and the previously loaded 'room_image' into a single list\n",
    "# so the multimodal model will analyze both the text and the image together.\n",
    "\n",
    "prompt1 = \"Describe this room:\"\n",
    "prompt2 = \"and recommend a type of chair that would fit in it\"\n",
    "\n",
    "contents = [\n",
    "    prompt1,\n",
    "    room_image,\n",
    "    prompt2\n",
    "]\n",
    "\n",
    "# We generate content from our multimodal model in a streaming fashion.\n",
    "# As soon as the model starts producing partial responses, we can read them in real-time.\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "# For clarity, we first print the prompt contents (text + image) so we can see\n",
    "# exactly what the model is receiving.\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "# Then, we print the model's response. The 'end=\"\"' argument ensures\n",
    "# we don't add unnecessary newlines between each streaming chunk.\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idEB0JFcbznD"
   },
   "source": [
    "### Generating recommendations based on provided images\n",
    "\n",
    "Instead of keeping the recommendation open, you can also provide a list of items for the model to choose from. Here you will download a few chair images and set them as options for the Gemini model to recommend from. This is particularly useful for retail companies who want to provide recommendations to users based on the kind of room they have, and the available items that the store offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_jMmwRiFcPF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We start by defining a list of URLs pointing to different chair images. \n",
    "# We'll later load these into Vertex AI Image objects for display and analysis.\n",
    "furniture_image_urls = [\n",
    "    \"https://storage.googleapis.com/github-repo/img/gemini/retail-recommendations/furnitures/cesar-couto-OB2F6CsMva8-unsplash.jpg\",\n",
    "    \"https://storage.googleapis.com/github-repo/img/gemini/retail-recommendations/furnitures/daniil-silantev-1P6AnKDw6S8-unsplash.jpg\",\n",
    "    \"https://storage.googleapis.com/github-repo/img/gemini/retail-recommendations/furnitures/ruslan-bardash-4kTbAMRAHtQ-unsplash.jpg\",\n",
    "    \"https://storage.googleapis.com/github-repo/img/gemini/retail-recommendations/furnitures/scopic-ltd-NLlWwR4d3qU-unsplash.jpg\",\n",
    "]\n",
    "\n",
    "# We convert each furniture image URL into a Vertex AI Image object \n",
    "# using the 'load_image_from_url' function. This allows us to include them \n",
    "# in our multimodal prompt.\n",
    "furniture_images = [\n",
    "    load_image_from_url(url) for url in furniture_image_urls\n",
    "]\n",
    "\n",
    "# We compose a list of prompt items (text + images). Labeling each chair \n",
    "# in the prompt helps the model reference them more accurately. \n",
    "# This approach reduces hallucinations and generally produces better results.\n",
    "contents = [\n",
    "    \"Consider the following chairs:\",\n",
    "    \"chair 1:\",\n",
    "    furniture_images[0],\n",
    "    \"chair 2:\",\n",
    "    furniture_images[1],\n",
    "    \"chair 3:\",\n",
    "    furniture_images[2],\n",
    "    \"chair 4:\",\n",
    "    furniture_images[3],\n",
    "    \"room:\",\n",
    "    room_image,  # Previously loaded room image\n",
    "    \"You are an interior designer. For each chair, explain whether it would be appropriate for the style of the room:\",\n",
    "]\n",
    "\n",
    "# We invoke 'generate_content' on our multimodal model, \n",
    "# passing in the prompt (contents) and asking for a streaming response.\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "# We display the prompt to see what we're sending to the model,\n",
    "# which shows both the text and inline images (if in a Jupyter environment).\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "# Lastly, we print the model's response in real-time as it's streamed.\n",
    "# Using end=\"\" ensures we don't add extra blank lines between chunks.\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mkw-DixOUYRc"
   },
   "source": [
    "You can also return the responses in JSON format, to make it easier to plug recommendations into a recommendation system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDBQufRYUYRc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We create a list of prompt items (text + images). We label each chair \n",
    "# to help the model reference them accurately. We also include a final instruction \n",
    "# asking the model to respond in JSON format, indicating whether each chair \n",
    "# would fit into the room, along with an explanation.\n",
    "\n",
    "contents = [\n",
    "    \"Consider the following chairs:\",\n",
    "    \"chair 1:\",\n",
    "    furniture_images[0],\n",
    "    \"chair 2:\",\n",
    "    furniture_images[1],\n",
    "    \"chair 3:\",\n",
    "    furniture_images[2],\n",
    "    \"chair 4:\",\n",
    "    furniture_images[3],\n",
    "    \"room:\",\n",
    "    room_image,\n",
    "    (\n",
    "        \"You are an interior designer. Return in JSON, for each chair, \"\n",
    "        \"whether it would fit in the room, with an explanation:\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# We generate responses using our multimodal model in a streaming fashion, \n",
    "# which allows us to retrieve partial results as they are produced.\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "# We print out the combined prompt first, which attempts to display \n",
    "# or print each item (text or images) in the notebook.\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "# We then print the model's responses as they are streamed. \n",
    "# The 'end=\"\"' parameter avoids adding extra newlines after each chunk.\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_uhGdZERToX"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook showed how you can easily build a multimodal recommendation system using Gemini for furniture, but you can also use the similar approach in:\n",
    "\n",
    "- recommending clothes based on an occasion or an image of the venue\n",
    "- recommending wallpaper based on the room and settings\n",
    "\n",
    "You may also want to explore how you can build a RAG (retrieval-augmented generation) system where you retrieve relevant images from your store inventory to users who can they use Gemini to help identify the most ideal choice from the various options provided, and also explain the rationale to users."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "multimodal_retail_recommendations.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
